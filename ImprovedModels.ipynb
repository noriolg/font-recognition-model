{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Font recognition - improved models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and train-validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data is loaded, observed and treated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('data/train_data.csv')\n",
    "train_labels = pd.read_csv('data/train_labels.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are factorized and a full dataframe is constructed adding the encoded values as the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoded, unique_labels = pd.factorize(train_labels['Font'])\n",
    "labels = pd.DataFrame(label_encoded, columns=['label'])\n",
    "df = pd.concat([train_data, labels], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and validation split is conducted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.iloc[:, :-1]\n",
    "Y = df.iloc[:, -1]\n",
    "x_train_df, x_valid_df, y_train_df, y_valid_df = train_test_split(X, Y, test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, the test data is loaded as well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "x_test_df = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now df has all the needed information. It will be transformed to a np.array for easier treatment within sklearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pre_norm = np.array(x_train_df)\n",
    "x_valid_pre_norm = np.array(x_valid_df)\n",
    "y_train = np.array(y_train_df)\n",
    "y_valid = np.array(y_valid_df)\n",
    "x_test_pre_norm = np.array(x_test_df)\n",
    "\n",
    "X_np = np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mean` and `std` are obtained from full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.sum(X_np, axis = 0) / X_np.shape[0]\n",
    "std = np.std(X_np, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement normalization function from Homework 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, mean, std):\n",
    "    \"\"\"Normalizes a given array X by columns \n",
    "    with the mean and std\"\"\"\n",
    "    X_out = np.zeros(X.shape)\n",
    "    X_out = (X - mean)/std\n",
    "    return X_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train_pre_norm, mean, std)\n",
    "x_valid = normalize(x_valid_pre_norm, mean, std)\n",
    "x_test = normalize(x_test_pre_norm, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to save submission csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function will be created that saves predictions as a csv with the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_as_csv(y_pred, file_name):\n",
    "    path = \"submissions/\"\n",
    "    status = 0\n",
    "    if len(y_pred) == 29221:\n",
    "        ids = np.arange(1,len(y_pred)+1,1)\n",
    "        pred_label = unique_labels[y_pred]\n",
    "        data = {'ID':ids, 'Font':pred_label} \n",
    "        submission = pd.DataFrame(data)\n",
    "        submission.to_csv(path + file_name + \".csv\", index = False)\n",
    "        status = 1\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=300, random_state=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nNetwork = MLPClassifier(random_state=1, max_iter=300)\n",
    "model_nNetwork.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is: 0.04342857142857143.\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model_nNetwork.predict(x_train)\n",
    "error = hamming_loss(y_train, y_pred_train)\n",
    "print('The training error is: ' + str(error) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation error is: 0.3304102564102564.\n"
     ]
    }
   ],
   "source": [
    "y_pred_valid = model_nNetwork.predict(x_valid)\n",
    "error = hamming_loss(y_valid, y_pred_valid)\n",
    "print('The validation error is: ' + str(error) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6695897435897435"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_score = 1- error\n",
    "predicted_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions with test set are computed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model_nNetwork.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_as_csv(y_pred_test, \"nNetwork_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.5, max_iter=300, random_state=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nNetwork = MLPClassifier(random_state=1, max_iter=300, alpha = 0.5, activation = 'logistic')\n",
    "model_nNetwork.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is: 0.320989010989011.\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model_nNetwork.predict(x_train)\n",
    "error = hamming_loss(y_train, y_pred_train)\n",
    "print('The training error is: ' + str(error) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation error is: 0.38271794871794873.\n"
     ]
    }
   ],
   "source": [
    "y_pred_valid = model_nNetwork.predict(x_valid)\n",
    "error = hamming_loss(y_valid, y_pred_valid)\n",
    "print('The validation error is: ' + str(error) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6172820512820513"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_score = 1- error\n",
    "predicted_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions with test set are computed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model_nNetwork.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_as_csv(y_pred_test, \"nNetwork_prediction2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network varying params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variation of parameters is tried to see f performance can be increased. It takes too much time to try out all compbinations so several of them were tried and results are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = ['logistic', 'tanh', 'relu', 'identity']\n",
    "solvers = ['adam']\n",
    "learning_rates = ['adaptive']\n",
    "alphas = np.linspace(start=1, stop =10, num = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_NN_performance = []\n",
    "columns = ['activation_function', 'solver', 'learning_rate', 'alpha', 'train_error', 'test_error', 'predicted_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "Activation function\n",
      "logistic\n",
      "\n",
      "======\n",
      "Solver\n",
      "adam\n",
      "=\n",
      "L rate\n",
      "adaptive\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "\n",
      "=============\n",
      "Activation function\n",
      "tanh\n",
      "\n",
      "======\n",
      "Solver\n",
      "adam\n",
      "=\n",
      "L rate\n",
      "adaptive\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "\n",
      "=============\n",
      "Activation function\n",
      "relu\n",
      "\n",
      "======\n",
      "Solver\n",
      "adam\n",
      "=\n",
      "L rate\n",
      "adaptive\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "\n",
      "=============\n",
      "Activation function\n",
      "identity\n",
      "\n",
      "======\n",
      "Solver\n",
      "adam\n",
      "=\n",
      "L rate\n",
      "adaptive\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "for activation_function in activation_functions:\n",
    "    print(\"\\n=============\")\n",
    "    print(\"Activation function\")\n",
    "    print(activation_function)\n",
    "    for solver in solvers:\n",
    "        print(\"\\n======\")\n",
    "        print(\"Solver\")\n",
    "        print(solver)\n",
    "        \n",
    "        for learning_rate in learning_rates:\n",
    "            print(\"=\")\n",
    "            print(\"L rate\")\n",
    "            print(learning_rate)\n",
    "            \n",
    "            for alpha in alphas:\n",
    "                print(alpha)\n",
    "                model_nNetwork = MLPClassifier(random_state=1, max_iter=600, alpha = alpha, activation = activation_function, solver = solver)\n",
    "                model_nNetwork.fit(x_train, y_train)\n",
    "                \n",
    "                y_pred_train = model_nNetwork.predict(x_train)\n",
    "                train_error = hamming_loss(y_train, y_pred_train)\n",
    "                \n",
    "                y_pred_valid = model_nNetwork.predict(x_valid)\n",
    "                test_error = hamming_loss(y_valid, y_pred_valid)\n",
    "                \n",
    "                predicted_score = 1- test_error\n",
    "                predicted_score\n",
    "                \n",
    "                rows_NN_performance.append(dict(zip(columns,[activation_function, solver, learning_rate, alpha, train_error, test_error, predicted_score ])))                \n",
    "\n",
    "df = pd.DataFrame(rows_NN_performance)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation_function</th>\n",
       "      <th>solver</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>alpha</th>\n",
       "      <th>train_error</th>\n",
       "      <th>test_error</th>\n",
       "      <th>predicted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.435033</td>\n",
       "      <td>0.463846</td>\n",
       "      <td>0.536154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.529516</td>\n",
       "      <td>0.543949</td>\n",
       "      <td>0.456051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.576110</td>\n",
       "      <td>0.584051</td>\n",
       "      <td>0.415949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.597077</td>\n",
       "      <td>0.607846</td>\n",
       "      <td>0.392154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.609890</td>\n",
       "      <td>0.619385</td>\n",
       "      <td>0.380615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.621099</td>\n",
       "      <td>0.629436</td>\n",
       "      <td>0.370564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.632066</td>\n",
       "      <td>0.637692</td>\n",
       "      <td>0.362308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.646330</td>\n",
       "      <td>0.650615</td>\n",
       "      <td>0.349385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.674066</td>\n",
       "      <td>0.678974</td>\n",
       "      <td>0.321026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.685319</td>\n",
       "      <td>0.689077</td>\n",
       "      <td>0.310923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.261253</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.347714</td>\n",
       "      <td>0.395744</td>\n",
       "      <td>0.604256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.403538</td>\n",
       "      <td>0.440872</td>\n",
       "      <td>0.559128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.442264</td>\n",
       "      <td>0.468564</td>\n",
       "      <td>0.531436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.491297</td>\n",
       "      <td>0.509231</td>\n",
       "      <td>0.490769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.513385</td>\n",
       "      <td>0.531179</td>\n",
       "      <td>0.468821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.529099</td>\n",
       "      <td>0.542974</td>\n",
       "      <td>0.457026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.542066</td>\n",
       "      <td>0.552564</td>\n",
       "      <td>0.447436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.547758</td>\n",
       "      <td>0.554513</td>\n",
       "      <td>0.445487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.556923</td>\n",
       "      <td>0.561590</td>\n",
       "      <td>0.438410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.241429</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.661538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.318088</td>\n",
       "      <td>0.378462</td>\n",
       "      <td>0.621538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.379495</td>\n",
       "      <td>0.426769</td>\n",
       "      <td>0.573231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.419736</td>\n",
       "      <td>0.451744</td>\n",
       "      <td>0.548256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.443473</td>\n",
       "      <td>0.471231</td>\n",
       "      <td>0.528769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.480066</td>\n",
       "      <td>0.498308</td>\n",
       "      <td>0.501692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.503055</td>\n",
       "      <td>0.518103</td>\n",
       "      <td>0.481897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.520813</td>\n",
       "      <td>0.535538</td>\n",
       "      <td>0.464462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.535297</td>\n",
       "      <td>0.549744</td>\n",
       "      <td>0.450256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.540330</td>\n",
       "      <td>0.552410</td>\n",
       "      <td>0.447590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>identity</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.511253</td>\n",
       "      <td>0.528923</td>\n",
       "      <td>0.471077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>identity</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.521473</td>\n",
       "      <td>0.540205</td>\n",
       "      <td>0.459795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>identity</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.525319</td>\n",
       "      <td>0.540564</td>\n",
       "      <td>0.459436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>identity</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.529846</td>\n",
       "      <td>0.544359</td>\n",
       "      <td>0.455641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>identity</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.535758</td>\n",
       "      <td>0.549333</td>\n",
       "      <td>0.450667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>identity</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.541473</td>\n",
       "      <td>0.548821</td>\n",
       "      <td>0.451179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>identity</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.546440</td>\n",
       "      <td>0.554718</td>\n",
       "      <td>0.445282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>identity</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.555077</td>\n",
       "      <td>0.564821</td>\n",
       "      <td>0.435179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>identity</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.557802</td>\n",
       "      <td>0.562667</td>\n",
       "      <td>0.437333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>identity</td>\n",
       "      <td>adam</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.562813</td>\n",
       "      <td>0.568308</td>\n",
       "      <td>0.431692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation_function solver learning_rate  alpha  train_error  test_error  \\\n",
       "0             logistic   adam      adaptive    1.0     0.435033    0.463846   \n",
       "1             logistic   adam      adaptive    2.0     0.529516    0.543949   \n",
       "2             logistic   adam      adaptive    3.0     0.576110    0.584051   \n",
       "3             logistic   adam      adaptive    4.0     0.597077    0.607846   \n",
       "4             logistic   adam      adaptive    5.0     0.609890    0.619385   \n",
       "5             logistic   adam      adaptive    6.0     0.621099    0.629436   \n",
       "6             logistic   adam      adaptive    7.0     0.632066    0.637692   \n",
       "7             logistic   adam      adaptive    8.0     0.646330    0.650615   \n",
       "8             logistic   adam      adaptive    9.0     0.674066    0.678974   \n",
       "9             logistic   adam      adaptive   10.0     0.685319    0.689077   \n",
       "10                tanh   adam      adaptive    1.0     0.261253    0.350000   \n",
       "11                tanh   adam      adaptive    2.0     0.347714    0.395744   \n",
       "12                tanh   adam      adaptive    3.0     0.403538    0.440872   \n",
       "13                tanh   adam      adaptive    4.0     0.442264    0.468564   \n",
       "14                tanh   adam      adaptive    5.0     0.491297    0.509231   \n",
       "15                tanh   adam      adaptive    6.0     0.513385    0.531179   \n",
       "16                tanh   adam      adaptive    7.0     0.529099    0.542974   \n",
       "17                tanh   adam      adaptive    8.0     0.542066    0.552564   \n",
       "18                tanh   adam      adaptive    9.0     0.547758    0.554513   \n",
       "19                tanh   adam      adaptive   10.0     0.556923    0.561590   \n",
       "20                relu   adam      adaptive    1.0     0.241429    0.338462   \n",
       "21                relu   adam      adaptive    2.0     0.318088    0.378462   \n",
       "22                relu   adam      adaptive    3.0     0.379495    0.426769   \n",
       "23                relu   adam      adaptive    4.0     0.419736    0.451744   \n",
       "24                relu   adam      adaptive    5.0     0.443473    0.471231   \n",
       "25                relu   adam      adaptive    6.0     0.480066    0.498308   \n",
       "26                relu   adam      adaptive    7.0     0.503055    0.518103   \n",
       "27                relu   adam      adaptive    8.0     0.520813    0.535538   \n",
       "28                relu   adam      adaptive    9.0     0.535297    0.549744   \n",
       "29                relu   adam      adaptive   10.0     0.540330    0.552410   \n",
       "30            identity   adam      adaptive    1.0     0.511253    0.528923   \n",
       "31            identity   adam      adaptive    2.0     0.521473    0.540205   \n",
       "32            identity   adam      adaptive    3.0     0.525319    0.540564   \n",
       "33            identity   adam      adaptive    4.0     0.529846    0.544359   \n",
       "34            identity   adam      adaptive    5.0     0.535758    0.549333   \n",
       "35            identity   adam      adaptive    6.0     0.541473    0.548821   \n",
       "36            identity   adam      adaptive    7.0     0.546440    0.554718   \n",
       "37            identity   adam      adaptive    8.0     0.555077    0.564821   \n",
       "38            identity   adam      adaptive    9.0     0.557802    0.562667   \n",
       "39            identity   adam      adaptive   10.0     0.562813    0.568308   \n",
       "\n",
       "    predicted_score  \n",
       "0          0.536154  \n",
       "1          0.456051  \n",
       "2          0.415949  \n",
       "3          0.392154  \n",
       "4          0.380615  \n",
       "5          0.370564  \n",
       "6          0.362308  \n",
       "7          0.349385  \n",
       "8          0.321026  \n",
       "9          0.310923  \n",
       "10         0.650000  \n",
       "11         0.604256  \n",
       "12         0.559128  \n",
       "13         0.531436  \n",
       "14         0.490769  \n",
       "15         0.468821  \n",
       "16         0.457026  \n",
       "17         0.447436  \n",
       "18         0.445487  \n",
       "19         0.438410  \n",
       "20         0.661538  \n",
       "21         0.621538  \n",
       "22         0.573231  \n",
       "23         0.548256  \n",
       "24         0.528769  \n",
       "25         0.501692  \n",
       "26         0.481897  \n",
       "27         0.464462  \n",
       "28         0.450256  \n",
       "29         0.447590  \n",
       "30         0.471077  \n",
       "31         0.459795  \n",
       "32         0.459436  \n",
       "33         0.455641  \n",
       "34         0.450667  \n",
       "35         0.451179  \n",
       "36         0.445282  \n",
       "37         0.435179  \n",
       "38         0.437333  \n",
       "39         0.431692  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network more hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.5, hidden_layer_sizes=(10, 6),\n",
       "              max_iter=500, random_state=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nNetwork = MLPClassifier(random_state=1, max_iter=500, alpha = 0.5, activation = 'logistic',  hidden_layer_sizes=(10, 6))\n",
    "model_nNetwork.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is: 0.4857582417582418.\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model_nNetwork.predict(x_train)\n",
    "error = hamming_loss(y_train, y_pred_train)\n",
    "print('The training error is: ' + str(error) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation error is: 0.5049743589743589.\n"
     ]
    }
   ],
   "source": [
    "y_pred_valid = model_nNetwork.predict(x_valid)\n",
    "error = hamming_loss(y_valid, y_pred_valid)\n",
    "print('The validation error is: ' + str(error) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49502564102564106"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_score = 1- error\n",
    "predicted_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import hamming_loss\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = ['logistic', 'tanh', 'relu', 'identity']\n",
    "solvers = ['adam','lbfgs', 'sgd']\n",
    "learning_rates = ['adaptive', 'invscaling']\n",
    "alphas = np.linspace(start=0.1, stop =3, num = 10).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = {'activation':('logistic', 'tanh', 'relu', 'identity'), 'solver':('adam','lbfgs', 'sgd'), 'learning_rate':('adaptive', 'invscaling'), 'alpha':alphas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_NN = MLPClassifier(max_iter=1000, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = GridSearchCV(neural_NN, parameters)\n",
    "clf = RandomizedSearchCV(neural_NN, distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\norio\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=MLPClassifier(max_iter=1000, random_state=1),\n",
       "                   param_distributions={'activation': ('logistic', 'tanh',\n",
       "                                                       'relu', 'identity'),\n",
       "                                        'alpha': [0.1, 0.42222222222222217,\n",
       "                                                  0.7444444444444444,\n",
       "                                                  1.0666666666666667,\n",
       "                                                  1.3888888888888888,\n",
       "                                                  1.711111111111111,\n",
       "                                                  2.033333333333333,\n",
       "                                                  2.3555555555555556,\n",
       "                                                  2.6777777777777776, 3.0],\n",
       "                                        'learning_rate': ('adaptive',\n",
       "                                                          'invscaling'),\n",
       "                                        'solver': ('adam', 'lbfgs', 'sgd')})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'sgd',\n",
       " 'learning_rate': 'adaptive',\n",
       " 'alpha': 1.0666666666666667,\n",
       " 'activation': 'relu'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is: 0.20125274725274725.\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = clf.predict(x_train)\n",
    "error = hamming_loss(y_train, y_pred_train)\n",
    "print('The training error is: ' + str(error) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation error is: 0.32743589743589746.\n"
     ]
    }
   ],
   "source": [
    "y_pred_valid = clf.predict(x_valid)\n",
    "error = hamming_loss(y_valid, y_pred_valid)\n",
    "print('The validation error is: ' + str(error) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6725641025641025"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_score = 1- error\n",
    "predicted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean_fit_time',\n",
       " 'mean_score_time',\n",
       " 'mean_test_score',\n",
       " 'param_activation',\n",
       " 'param_alpha',\n",
       " 'param_learning_rate',\n",
       " 'param_solver',\n",
       " 'params',\n",
       " 'rank_test_score',\n",
       " 'split0_test_score',\n",
       " 'split1_test_score',\n",
       " 'split2_test_score',\n",
       " 'split3_test_score',\n",
       " 'split4_test_score',\n",
       " 'std_fit_time',\n",
       " 'std_score_time',\n",
       " 'std_test_score']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is: 0.022395604395604396.\n",
      "The validation error is: 0.2753333333333333.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7246666666666667"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nNetwork = MLPClassifier(random_state=1, max_iter=1000, alpha = 1.38, activation = 'relu', learning_rate = 'adaptive', solver = 'sgd', hidden_layer_sizes=(200,100,100,50) )\n",
    "model_nNetwork.fit(x_train, y_train)\n",
    "\n",
    "y_pred_train = model_nNetwork.predict(x_train)\n",
    "error = hamming_loss(y_train, y_pred_train)\n",
    "print('The training error is: ' + str(error) + '.')\n",
    "\n",
    "y_pred_valid = model_nNetwork.predict(x_valid)\n",
    "error = hamming_loss(y_valid, y_pred_valid)\n",
    "print('The validation error is: ' + str(error) + '.')\n",
    "\n",
    "predicted_score = 1- error\n",
    "predicted_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
